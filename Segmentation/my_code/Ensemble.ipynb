{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b736f5fc-3b98-4066-a1ab-6700c97e54f0",
   "metadata": {},
   "source": [
    "### Ensemble \n",
    "\n",
    "- TTA 기반 두개 모델 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51689041-245d-4f34-b768-607dce3a2e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5f9f159ab61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minference_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mModels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSwinTransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Datasets'"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import pandas as pd\n",
    "from Utils import inference_view\n",
    "from Datasets import *\n",
    "from Models import *\n",
    "from SwinTransformer import *\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fe0490-313b-47a0-93b8-f9335e6cb28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/837 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [19:05<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "weights = [\n",
    "    '/opt/ml/code/saved/mIoU_swin_small_512_stepRL.pt',\n",
    "    '/opt/ml/code/saved/mIoU_swin_small_512_stepRL_CLAHE.pt'\n",
    "]\n",
    "\n",
    "\n",
    "def get_pretrained_model(path):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SwinTransformerSmall()\n",
    "    model_path = path\n",
    "\n",
    "    # best model 불러오기\n",
    "    print(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "raw_model, transformed_model = [get_pretrained_model(weight) for weight in weights ]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "])\n",
    "raw_data = RecylceDatasets(data_dir='../input/data/test.json', mode='test', transform=test_transform,\n",
    "                              # resize=512\n",
    "                              )\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.CLAHE(p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "transform_data = RecylceDatasets(data_dir='../input/data/test.json', mode='test', transform=test_transform,\n",
    "                              # resize=512\n",
    "                              )\n",
    "\n",
    "res = []\n",
    "\n",
    "resize = A.Compose([\n",
    "    A.Resize(256,256)\n",
    "])\n",
    "\n",
    "tmp = raw_data[0][0].detach().cpu().numpy()\n",
    "for i in tqdm(range(len(raw_data))):\n",
    "    out_1,_= inference_view(raw_model,idx=i, dataset=raw_data, result_plot=False, confidence_plot=False)\n",
    "    out_1 = out_1[0].detach().cpu().numpy()\n",
    "    \n",
    "    out_2,_ = inference_view(transformed_model, idx=i, dataset=transform_data, result_plot=False, confidence_plot=False)\n",
    "    out_2 = out_2[0].detach().cpu().numpy()\n",
    "    \n",
    "    out = out_1 + out_2\n",
    "    out = out.argmax(axis=0)\n",
    "    resized = resize(image=tmp, mask=out)\n",
    "    out = resized['mask']\n",
    "    out = out.reshape((1,256*256))\n",
    "    res.append(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afa7f4-45a3-4a73-9e21-3a99309c4f05",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 다중모델, 동일 data 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc625d-279e-4f57-8f49-6ccaa27971fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import pandas as pd\n",
    "from Utils import inference_view # 필수입니다!\n",
    "from Datasets import * # 불필요..\n",
    "from Models import * # 불필요\n",
    "from SwinTransformer import * # 불필요\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f3a4cb6-1719-41d4-9ac2-58ed3219501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/837 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [20:39<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "weights = [\n",
    "    '/opt/ml/code/saved/0fold_best_model.pt',\n",
    "    '/opt/ml/code/saved/1fold_best_model.pt',\n",
    "    '/opt/ml/code/saved/2fold_best_model.pt',\n",
    "    '/opt/ml/code/saved/3fold_best_model.pt',\n",
    "    '/opt/ml/code/saved/4fold_best_model.pt'\n",
    "]\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "models = smp.DeepLabV3Plus(\n",
    "    encoder_name='resnext50_32x4d',\n",
    "    encoder_weights='swsl',\n",
    "    in_channels=3,\n",
    "    classes=12,\n",
    ")\n",
    "\n",
    "\n",
    "def get_pretrained_model(model, path):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model \n",
    "    model_path = path\n",
    "\n",
    "    # best model 불러오기\n",
    "    print(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "# 학습된 모델들 불러오기\n",
    "trained_models = [get_pretrained_model(models, weights[i]) for i in range(len(weights))]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    # A.CLAHE(p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "raw_data = RecylceDatasets(data_dir='../input/data/test.json', mode='test', transform=test_transform,\n",
    "                              # resize=512\n",
    "                              )\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.CLAHE(p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "transform_data = RecylceDatasets(data_dir='../input/data/test.json', mode='test', transform=test_transform,\n",
    "                              # resize=512\n",
    "                              )\n",
    "\n",
    "res = []\n",
    "\n",
    "resize = A.Compose([\n",
    "    A.Resize(256,256)\n",
    "])\n",
    "\n",
    "tmp = raw_data[0][0].detach().cpu().numpy()\n",
    "\n",
    "for i in range(len(raw_data)):\n",
    "    out = np.zeros((12,256,256))\n",
    "    for m in trained_models:\n",
    "        out_,_= inference_view(m, idx=i, dataset=raw_data, result_plot=False, confidence_plot=False)\n",
    "        out_ = out_1[0].detach().cpu().numpy()\n",
    "        out += out_\n",
    "    out = out.argmax(axis=0)\n",
    "    resized = resize(image=tmp, mask=out)\n",
    "    out = resized['mask']\n",
    "    out = out.reshape((1,256*256))\n",
    "    res.append(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae7dfc5-4709-43e3-8dbf-d3f877d7a760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed18649290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(res[50].reshape(256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4ee4d-26cb-40b8-a52e-b1d7d30dd057",
   "metadata": {},
   "source": [
    "## Save_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1f9396-eea2-4299-9add-ed5f50796dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 21\n",
    "file_name = 'Swin_small_Base_TTA_CHALE'\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "\n",
    "# PredictionString 대입\n",
    "for i, r in enumerate(res):\n",
    "    submission = submission.append(\n",
    "        {\"image_id\": raw_data.coco.loadImgs(raw_data.coco.getImgIds(i))[0]['file_name'], \n",
    "         \"PredictionString\": ' '.join(str(e) for e in r.tolist()[0])},\n",
    "        ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(f\"./submission/{n}ensemble\" + file_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c13690-ce87-4dbb-959c-dcba128919e6",
   "metadata": {},
   "source": [
    "### Multi scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d48f88-3dcc-49b2-a71b-bc0a9f405109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import pandas as pd\n",
    "from Utils import inference_view\n",
    "from Datasets import *\n",
    "from Models import *\n",
    "from SwinTransformer import *\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfa28ac2-0b1a-4726-b0cd-aaee626bf96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/837 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "load_complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [09:26<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights = [\n",
    "    '/opt/ml/code/saved/mIoU_swin_Small_allData.pt',\n",
    "#     '/opt/ml/code/saved/mIoU_swin_small_512_stepRL_CLAHE.pt'\n",
    "]\n",
    "\n",
    "\n",
    "def get_pretrained_model(path):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SwinTransformerSmall()\n",
    "    model_path = path\n",
    "\n",
    "    # best model 불러오기\n",
    "    print(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "model = get_pretrained_model(weights[0])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "transform = A.Compose([\n",
    "    # A.CLAHE(p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "scale_size = [0.5,0.75,1,1.25,1.5]\n",
    "datasets = [RecylceDatasets(data_dir='../input/data/test.json', mode='test', transform=transform, resize=int(512*i)) for i in scale_size] \n",
    "\n",
    "\n",
    "res = []\n",
    "resize = A.Compose([\n",
    "    A.Resize(256,256)\n",
    "])\n",
    "tmp = datasets[0][0][0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(datasets[0]))):\n",
    "    out = np.zeros((256,256,12))\n",
    "    for dataset in datasets:\n",
    "        out_1,_= inference_view(model,idx=i, dataset=dataset, result_plot=False, confidence_plot=False) # 12,512,512\n",
    "        out_1 = out_1[0].detach().cpu().numpy().transpose((1,2,0)) # 512, 512, 12\n",
    "        resized = resize(image=out_1, mask=out_1)# 256,256,12\n",
    "        t = resized['image'] # 256,256,12\n",
    "        out += t\n",
    "        \n",
    "    \n",
    "    out = out.transpose((2,0,1)).argmax(axis=0) # 12,512,512\n",
    "    out = out.reshape((1,256*256))\n",
    "    res.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02010a5-992b-4b09-ac69-802c5c214016",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 23\n",
    "file_name = 'Swin_small_512_multi_scale'\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "\n",
    "# PredictionString 대입\n",
    "for i, r in enumerate(res):\n",
    "    submission = submission.append(\n",
    "        {\"image_id\": datasets[0].coco.loadImgs(datasets[0].coco.getImgIds(i))[0]['file_name'], \n",
    "         \"PredictionString\": ' '.join(str(e) for e in r.tolist()[0])},\n",
    "        ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(f\"./submission/{n}ensemble\" + file_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb627cb-87e7-460d-9a27-1773581767f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
